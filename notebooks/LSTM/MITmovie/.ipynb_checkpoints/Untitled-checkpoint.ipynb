{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################\n",
    "###  Fetch data and Preprocess  ###\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus.reader import *\n",
    "from nltk import flatten\n",
    "\n",
    "\n",
    "# Read the dataset \n",
    "train = ConllCorpusReader('data/MITMovieCorpus',\n",
    "                          'train.txt',\n",
    "                          ['pos', 'words'])\n",
    "\n",
    "\n",
    "test = ConllCorpusReader('data/MITMovieCorpus', \n",
    "                         'test.txt',\n",
    "                          ['pos', 'words'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training records: 7816\n",
      "Number of testing records: 1953\n",
      "[('steve', 'B-Actor'), ('mcqueen', 'I-Actor'), ('provided', 'O'), ('a', 'O'), ('thrilling', 'B-Plot'), ('motorcycle', 'I-Plot'), ('chase', 'I-Plot'), ('in', 'I-Plot'), ('this', 'I-Plot'), ('greatest', 'B-Opinion'), ('of', 'I-Opinion'), ('all', 'I-Opinion'), ('ww', 'B-Plot'), ('2', 'I-Plot'), ('prison', 'I-Plot'), ('escape', 'I-Plot'), ('movies', 'I-Plot')]\n"
     ]
    }
   ],
   "source": [
    "##couldnt find the method to get NE so it is represented as POS\n",
    "sentences = train.tagged_sents() + test.tagged_sents()\n",
    "test_sentences = test.tagged_sents() \n",
    "\n",
    "print(\"Number of training records: \" + str(len(train.tagged_sents())))\n",
    "print(\"Number of testing records: \" + str(len(test.tagged_sents())))\n",
    "print(sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "biggest sentence has 76 words\n"
     ]
    }
   ],
   "source": [
    "largest_sen = max(len(sen) for sen in sentences)\n",
    "print('biggest sentence has {} words'.format(largest_sen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1.600e+01, 1.140e+02, 1.740e+02, 3.122e+03, 3.341e+03, 1.313e+03,\n",
       "        9.220e+02, 4.570e+02, 1.320e+02, 7.100e+01, 5.100e+01, 2.800e+01,\n",
       "        1.600e+01, 5.000e+00, 1.000e+00, 2.000e+00, 1.000e+00, 1.000e+00,\n",
       "        1.000e+00, 1.000e+00]),\n",
       " array([ 3.  ,  6.65, 10.3 , 13.95, 17.6 , 21.25, 24.9 , 28.55, 32.2 ,\n",
       "        35.85, 39.5 , 43.15, 46.8 , 50.45, 54.1 , 57.75, 61.4 , 65.05,\n",
       "        68.7 , 72.35, 76.  ]),\n",
       " <a list of 20 Patch objects>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(\"ggplot\")\n",
    "\n",
    "plt.hist([len(sen) for sen in sentences], bins=20)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "As seen in the graph above, majority of the sentences contain less than 40 words even though the there is one sentences that contains  words. We will truncate all sentences larger than 30 words and the sentences that hav less than 30 words will be padded. This is done because, the input that is passed into lstm model has to be of same length. If all sentences are forced to be 334len then most of the words will just be padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n",
      "['i', 'm', 'thinking', 'of', 'the', 'epic', 'movie', 'based', 'on', 'the', 'book', 'by', 'j', 'r', 'r', 'tolkien', 'featuring', 'hobbits', 'wizards', 'elves', 'and', 'a', 'dark', 'magic', 'xxxPADDINGxxx', 'xxxPADDINGxxx', 'xxxPADDINGxxx', 'xxxPADDINGxxx', 'xxxPADDINGxxx', 'xxxPADDINGxxx']\n"
     ]
    }
   ],
   "source": [
    "max_len = 30\n",
    "\n",
    "X = []\n",
    "padded_sent = []\n",
    "x_index = 0\n",
    "for s in sentences:\n",
    "    for i in range(max_len):\n",
    "        if i >= len(s):\n",
    "            padded_sent.append(\"xxxPADDINGxxx\")\n",
    "        else:\n",
    "            padded_sent.append(s[i][0])\n",
    "    \n",
    "    X.append(padded_sent)\n",
    "    padded_sent = []\n",
    "print (len(X[53]))\n",
    "print(X[53])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "For the named entities, we will encode them as one hot vectors because they are the values that our model will output to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n"
     ]
    }
   ],
   "source": [
    "tags = []\n",
    "for s in sentences:\n",
    "    for w in s:\n",
    "        tags.append(w[1])\n",
    "tags = list(set(tags))\n",
    "num_tags = len(tags)\n",
    "\n",
    "print(num_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('what', 'O'), ('is', 'O'), ('the', 'O'), ('movie', 'O'), ('about', 'O'), ('luke', 'B-Character_Name'), ('skywalker', 'I-Character_Name'), ('breaking', 'B-Plot'), ('free', 'I-Plot'), ('from', 'I-Plot'), ('his', 'I-Plot'), ('aunt', 'I-Plot'), ('and', 'I-Plot'), ('uncle', 'I-Plot'), ('to', 'I-Plot'), ('rescue', 'I-Plot'), ('princess', 'B-Character_Name'), ('leia', 'I-Character_Name')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "tags2index = {t:i for i,t in enumerate(tags)}\n",
    "y = [[tags2index[w[1]] for w in s] for s in sentences]\n",
    "y = pad_sequences(maxlen=max_len, sequences=y, padding=\"post\", value=tags2index[\"O\"])\n",
    "\n",
    "y = [to_categorical(i, num_classes = num_tags) for i in y]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to change our words into numbers actually vectors to be specific (this is also also called embedding). Keras has a functional to do this but it requires us to map each unique word to a unique integer which will be passed into the function. The function takes:\n",
    "\n",
    "__input_dim:__ The total no. of unique words in our corpus. <br>\n",
    "__output_dim:__ Size of the vector that we want. <br>\n",
    "__input_length:__ Size of each spicific sentence (60). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3713, 7051, 5085, 6827, 1797, 1680, 1745, 6519, 9460, 11724, 3657, 5563, 9041, 502, 3262, 6445, 2516, 4188, 3657, 6229, 2435, 2907, 1954, 10026, 4561, 3731, 10575, 10575, 10575, 10575]\n"
     ]
    }
   ],
   "source": [
    "words = []\n",
    "for s in X:\n",
    "    for w in s:\n",
    "        words.append(w)\n",
    "unique_words = list(set(words))\n",
    "num_words = len(unique_words)\n",
    "\n",
    "words2index = { w: i for i,w in enumerate(unique_words) }\n",
    "X = [[words2index[w] for w in s] for s in X]\n",
    "# X = pad_sequences(maxlen=max_len, sequences=X, padding=\"post\", value=tags2index[\"xxxPADDINGxxx\"])\n",
    "print(X[1])\n",
    "\n",
    "from keras.models import Model, Input\n",
    "from keras.layers.merge import add\n",
    "from keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional, Lambda\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_TRAIN, X_TEST, Y_TRAIN, Y_TEST = train_test_split(X,y, test_size=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model, Input, Sequential\n",
    "from keras.layers.merge import add\n",
    "from keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional, Lambda, Flatten\n",
    "import numpy as np\n",
    "\n",
    "input = Input(shape=(max_len,))\n",
    "embedding = Embedding(input_dim = num_words, output_dim = 70, input_length = max_len)(input)\n",
    "\n",
    "x = Bidirectional(LSTM(units=512, return_sequences=True,\n",
    "                       recurrent_dropout=0.2, dropout=0.2))(embedding)\n",
    "x_rnn = Bidirectional(LSTM(units=512, return_sequences=False,\n",
    "                           recurrent_dropout=0.2, dropout=0.2))(x)\n",
    "x = add([x, x_rnn])  # residual connection to the first biLSTM\n",
    "out = TimeDistributed(Dense(num_tags, activation=\"softmax\"))(x)\n",
    "model = Model(input, out)\n",
    "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "\n",
    "model.compile('adam', 'binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "X_TRAIN, X_TEST = np.array(X_TRAIN), np.array(X_TEST)\n",
    "\n",
    "\n",
    "# print('x_train shape:', X_TRAIN.shape)\n",
    "# print('x_test shape:', X_TEST.shape)\n",
    "# print('out = ', out.shape)\n",
    "\n",
    "# print()\n",
    "# print()\n",
    "\n",
    "# print(X_TRAIN)\n",
    "\n",
    "# print()\n",
    "# print()\n",
    "# print()\n",
    "\n",
    "\n",
    "# print(np.array(Y_TRAIN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8792 samples, validate on 977 samples\n",
      "Epoch 1/4\n",
      "8792/8792 [==============================] - 575s 65ms/step - loss: 0.0590 - accuracy: 0.9802 - val_loss: 0.0445 - val_accuracy: 0.9861\n",
      "Epoch 2/4\n",
      "8792/8792 [==============================] - 634s 72ms/step - loss: 0.0351 - accuracy: 0.9886 - val_loss: 0.0317 - val_accuracy: 0.9898\n",
      "Epoch 3/4\n",
      "8792/8792 [==============================] - 585s 67ms/step - loss: 0.0253 - accuracy: 0.9915 - val_loss: 0.0271 - val_accuracy: 0.9910\n",
      "Epoch 4/4\n",
      "8792/8792 [==============================] - 560s 64ms/step - loss: 0.0210 - accuracy: 0.9927 - val_loss: 0.0254 - val_accuracy: 0.9913\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1a4fc1aa90>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 32\n",
    "\n",
    "print('Training...')\n",
    "model.fit(X_TRAIN, np.array(Y_TRAIN),\n",
    "          validation_data=(np.array(X_TEST), np.array(Y_TEST)),\n",
    "          batch_size=batch_size,\n",
    "          epochs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word           (True ): Pred\n",
      "arnold        : I-Actor\n",
      "the           : O\n",
      "governator    : O\n",
      "schwarzenegger: B-Genre\n",
      "s             : O\n",
      "1982          : B-Year\n",
      "classic       : I-Genre\n",
      "follows       : O\n",
      "the           : O\n",
      "adventures    : I-Plot\n",
      "of            : I-Plot\n",
      "the           : I-Plot\n",
      "eponymous     : I-Plot\n",
      "character     : I-Plot\n",
      "in            : I-Plot\n",
      "a             : I-Plot\n",
      "world         : I-Plot\n",
      "of            : I-Plot\n",
      "dark          : I-Plot\n",
      "magic         : I-Plot\n",
      "and           : I-Plot\n",
      "savagery      : I-Plot\n",
      "xxxPADDINGxxx : O\n",
      "xxxPADDINGxxx : O\n",
      "xxxPADDINGxxx : O\n",
      "xxxPADDINGxxx : O\n",
      "xxxPADDINGxxx : O\n",
      "xxxPADDINGxxx : O\n",
      "xxxPADDINGxxx : O\n",
      "xxxPADDINGxxx : O\n"
     ]
    }
   ],
   "source": [
    "i = 27\n",
    "p = model.predict(np.array([X_TEST[i]]))\n",
    "p = np.argmax(p, axis=-1)\n",
    "\n",
    "print(\"{:14} ({:5}): {}\".format(\"Word\", \"True\", \"Pred\"))\n",
    "for w,pred in zip(X_TEST[i],p[0]):\n",
    "    print(\"{:14}: {}\".format(unique_words[w],tags[pred]))\n",
    "    \n",
    "#print(len(X_TRAIN[36]))\n",
    "#print(words[X_TRAIN[4][1]])\n",
    "\n",
    "model.save('inital.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
